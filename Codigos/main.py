from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from lector_pdf import retriever  # Activa esta l√≠nea

model = OllamaLLM(model="gemma3:1b")

template = """
Eres un experto resolviendo preguntas frecuentes en una universidad de alto prestigio
Quiero que sea lo mas corto y claro posible
Aqu√≠ se encuentra la informaci√≥n necesaria: {informacion}

Aqu√≠ est√° la pregunta para responder: {pregunta}
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

while True:
    pregunta = input("Realiza una pregunta ('q' para salir): ")
    if pregunta.lower() == "q":
        break
    informacion = retriever(pregunta)
    #print("üîé CONTEXTO EXTRA√çDO:\n", informacion)
    
    respuesta = chain.invoke({"informacion": informacion, "pregunta": pregunta})
    print("\nüß† Respuesta:\n", respuesta)